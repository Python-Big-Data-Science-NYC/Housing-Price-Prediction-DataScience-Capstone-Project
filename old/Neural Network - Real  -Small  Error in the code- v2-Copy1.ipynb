{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ironfrown/deep-learning-house-price-prediction-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kc_data = pd.read_csv(r'./input/kc_house_data.csv')\n",
    "kc_data = pd.read_pickle('df18featuesy.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        RDEATH2016  RNATURALINC2016  RINTERNATIONALMIG2016  RDOMESTICMIG2016  \\\n",
      "count  8757.000000      8757.000000            8757.000000       8757.000000   \n",
      "mean      9.850174         1.614091               1.219975          0.030908   \n",
      "std       2.426470         3.892970               1.663726          8.960091   \n",
      "min       1.522920       -12.437011              -0.857082        -68.325997   \n",
      "25%       8.256732        -0.815895               0.243078         -4.889880   \n",
      "50%       9.880820         1.351976               0.661376         -0.612531   \n",
      "75%      11.493437         3.787800               1.548359          4.480376   \n",
      "max      21.594684        27.051161              17.903389         72.957977   \n",
      "\n",
      "        RBIRTH2016        HPIUS    mortgrate        unemp          cpi  \\\n",
      "count  8757.000000  8757.000000  8757.000000  8757.000000  8757.000000   \n",
      "mean     11.464265     0.003773    -0.009783    -0.008234     0.001952   \n",
      "std       2.347001     0.003826     0.019136     0.016942     0.003314   \n",
      "min       3.881402    -0.002834    -0.038860    -0.023529    -0.004706   \n",
      "25%       9.987023     0.000187    -0.022727    -0.021505     0.001653   \n",
      "50%      11.338200     0.004791    -0.010101    -0.020000     0.002957   \n",
      "75%      12.692169     0.006039     0.010616     0.012658     0.004400   \n",
      "max      30.045173     0.009195     0.017910     0.017857     0.004763   \n",
      "\n",
      "                AL     ...       RNATURALINC2016 (t-2)  \\\n",
      "count  8757.000000     ...                 8757.000000   \n",
      "mean      0.022725     ...                    1.613553   \n",
      "std       0.149033     ...                    3.892286   \n",
      "min       0.000000     ...                  -12.437011   \n",
      "25%       0.000000     ...                   -0.815895   \n",
      "50%       0.000000     ...                    1.352073   \n",
      "75%       0.000000     ...                    3.782441   \n",
      "max       1.000000     ...                   27.051161   \n",
      "\n",
      "       RINTERNATIONALMIG2016 (t-2)  RDOMESTICMIG2016 (t-2)  RBIRTH2016 (t-2)  \\\n",
      "count                  8757.000000             8757.000000       8757.000000   \n",
      "mean                      1.220258                0.035685         11.463889   \n",
      "std                       1.663533                8.960235          2.346721   \n",
      "min                      -0.857082              -68.325997          3.881402   \n",
      "25%                       0.243314               -4.884238          9.987023   \n",
      "50%                       0.661649               -0.610814         11.338200   \n",
      "75%                       1.548359                4.490023         12.691846   \n",
      "max                      17.903389               72.957977         30.045173   \n",
      "\n",
      "       price (t-2)  HPIUS (t-2)  mortgrate (t-2)  unemp (t-2)    cpi (t-2)  \\\n",
      "count  8757.000000  8757.000000      8757.000000  8757.000000  8757.000000   \n",
      "mean     -0.009675     0.003771        -0.009775    -0.008235     0.001952   \n",
      "std       0.066179     0.003827         0.019138     0.016942     0.003314   \n",
      "min      -0.359436    -0.002834        -0.038860    -0.023529    -0.004706   \n",
      "25%      -0.049791     0.000187        -0.022727    -0.021505     0.001653   \n",
      "50%      -0.001557     0.004791        -0.010101    -0.020000     0.002957   \n",
      "75%       0.038709     0.006039         0.010616     0.012658     0.004400   \n",
      "max       0.107404     0.009195         0.017910     0.017857     0.004763   \n",
      "\n",
      "             price  \n",
      "count  8757.000000  \n",
      "mean     -0.009661  \n",
      "std       0.066180  \n",
      "min      -0.359436  \n",
      "25%      -0.049791  \n",
      "50%      -0.001548  \n",
      "75%       0.038735  \n",
      "max       0.107404  \n",
      "\n",
      "[8 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "# kc_data_org['sale_yr'] = pd.to_numeric(kc_data_org.date.str.slice(0, 4))\n",
    "# kc_data_org['sale_month'] = pd.to_numeric(kc_data_org.date.str.slice(4, 6))\n",
    "# kc_data_org['sale_day'] = pd.to_numeric(kc_data_org.date.str.slice(6, 8))\n",
    "\n",
    "# kc_data = pd.DataFrame(kc_data_org, columns=[\n",
    "#         'sale_yr','sale_month','sale_day',\n",
    "#         'bedrooms','bathrooms','sqft_living','sqft_lot','floors',\n",
    "#         'condition','grade','sqft_above','sqft_basement','yr_built',\n",
    "#         'zipcode','lat','long','sqft_living15','sqft_lot15','price'])\n",
    "\n",
    "label_col = 'price'\n",
    "\n",
    "print(kc_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RDEATH2016</th>\n",
       "      <th>RNATURALINC2016</th>\n",
       "      <th>RINTERNATIONALMIG2016</th>\n",
       "      <th>RDOMESTICMIG2016</th>\n",
       "      <th>RBIRTH2016</th>\n",
       "      <th>HPIUS</th>\n",
       "      <th>mortgrate</th>\n",
       "      <th>unemp</th>\n",
       "      <th>cpi</th>\n",
       "      <th>AL</th>\n",
       "      <th>...</th>\n",
       "      <th>RNATURALINC2016 (t-2)</th>\n",
       "      <th>RINTERNATIONALMIG2016 (t-2)</th>\n",
       "      <th>RDOMESTICMIG2016 (t-2)</th>\n",
       "      <th>RBIRTH2016 (t-2)</th>\n",
       "      <th>price (t-2)</th>\n",
       "      <th>HPIUS (t-2)</th>\n",
       "      <th>mortgrate (t-2)</th>\n",
       "      <th>unemp (t-2)</th>\n",
       "      <th>cpi (t-2)</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>11.632586</td>\n",
       "      <td>0.621868</td>\n",
       "      <td>-0.146322</td>\n",
       "      <td>-1.133994</td>\n",
       "      <td>12.254454</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.345519</td>\n",
       "      <td>0.327282</td>\n",
       "      <td>5.836523</td>\n",
       "      <td>11.563952</td>\n",
       "      <td>-0.024568</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>-0.077495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>9.871136</td>\n",
       "      <td>3.058662</td>\n",
       "      <td>-0.052136</td>\n",
       "      <td>1.807391</td>\n",
       "      <td>12.929798</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.962947</td>\n",
       "      <td>0.989585</td>\n",
       "      <td>14.768069</td>\n",
       "      <td>11.831760</td>\n",
       "      <td>-0.093731</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>-0.086218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>12.950211</td>\n",
       "      <td>-1.174622</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>-2.496072</td>\n",
       "      <td>11.775588</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621868</td>\n",
       "      <td>-0.146322</td>\n",
       "      <td>-1.133994</td>\n",
       "      <td>12.254454</td>\n",
       "      <td>-0.077495</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>-0.194399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>11.911623</td>\n",
       "      <td>-3.419789</td>\n",
       "      <td>0.076849</td>\n",
       "      <td>6.109510</td>\n",
       "      <td>8.491835</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.058662</td>\n",
       "      <td>-0.052136</td>\n",
       "      <td>1.807391</td>\n",
       "      <td>12.929798</td>\n",
       "      <td>-0.086218</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>-0.148970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>10.092682</td>\n",
       "      <td>3.397340</td>\n",
       "      <td>1.728471</td>\n",
       "      <td>0.993374</td>\n",
       "      <td>13.490022</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.174622</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>-2.496072</td>\n",
       "      <td>11.775588</td>\n",
       "      <td>-0.194399</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>0.010616</td>\n",
       "      <td>-0.021505</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>-0.076972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            RDEATH2016  RNATURALINC2016  RINTERNATIONALMIG2016  \\\n",
       "Year                                                             \n",
       "2011-01-01   11.632586         0.621868              -0.146322   \n",
       "2011-01-01    9.871136         3.058662              -0.052136   \n",
       "2011-01-01   12.950211        -1.174622               0.910332   \n",
       "2011-01-01   11.911623        -3.419789               0.076849   \n",
       "2011-01-01   10.092682         3.397340               1.728471   \n",
       "\n",
       "            RDOMESTICMIG2016  RBIRTH2016     HPIUS  mortgrate     unemp  \\\n",
       "Year                                                                      \n",
       "2011-01-01         -1.133994   12.254454 -0.002834   0.010616 -0.021505   \n",
       "2011-01-01          1.807391   12.929798 -0.002834   0.010616 -0.021505   \n",
       "2011-01-01         -2.496072   11.775588 -0.002834   0.010616 -0.021505   \n",
       "2011-01-01          6.109510    8.491835 -0.002834   0.010616 -0.021505   \n",
       "2011-01-01          0.993374   13.490022 -0.002834   0.010616 -0.021505   \n",
       "\n",
       "                 cpi  AL    ...     RNATURALINC2016 (t-2)  \\\n",
       "Year                        ...                             \n",
       "2011-01-01  0.004763   1    ...                  2.345519   \n",
       "2011-01-01  0.004763   1    ...                  1.962947   \n",
       "2011-01-01  0.004763   1    ...                  0.621868   \n",
       "2011-01-01  0.004763   1    ...                  3.058662   \n",
       "2011-01-01  0.004763   1    ...                 -1.174622   \n",
       "\n",
       "            RINTERNATIONALMIG2016 (t-2)  RDOMESTICMIG2016 (t-2)  \\\n",
       "Year                                                              \n",
       "2011-01-01                     0.327282                5.836523   \n",
       "2011-01-01                     0.989585               14.768069   \n",
       "2011-01-01                    -0.146322               -1.133994   \n",
       "2011-01-01                    -0.052136                1.807391   \n",
       "2011-01-01                     0.910332               -2.496072   \n",
       "\n",
       "            RBIRTH2016 (t-2)  price (t-2)  HPIUS (t-2)  mortgrate (t-2)  \\\n",
       "Year                                                                      \n",
       "2011-01-01         11.563952    -0.024568    -0.002834         0.010616   \n",
       "2011-01-01         11.831760    -0.093731    -0.002834         0.010616   \n",
       "2011-01-01         12.254454    -0.077495    -0.002834         0.010616   \n",
       "2011-01-01         12.929798    -0.086218    -0.002834         0.010616   \n",
       "2011-01-01         11.775588    -0.194399    -0.002834         0.010616   \n",
       "\n",
       "            unemp (t-2)  cpi (t-2)     price  \n",
       "Year                                          \n",
       "2011-01-01    -0.021505   0.004763 -0.077495  \n",
       "2011-01-01    -0.021505   0.004763 -0.086218  \n",
       "2011-01-01    -0.021505   0.004763 -0.194399  \n",
       "2011-01-01    -0.021505   0.004763 -0.148970  \n",
       "2011-01-01    -0.021505   0.004763 -0.076972  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_part=.6, validate_part=.2, test_part=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    total_size = train_part + validate_part + test_part\n",
    "    train_percent = train_part / total_size\n",
    "    validate_percent = validate_part / total_size\n",
    "    test_percent = test_part / total_size\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = perm[:train_end]\n",
    "    validate = perm[train_end:validate_end]\n",
    "    test = perm[validate_end:]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, valid_size, test_size = (70, 30, 0)\n",
    "kc_train, kc_valid, kc_test = train_validate_test_split(kc_data, \n",
    "                              train_part=train_size, \n",
    "                              validate_part=valid_size,\n",
    "                              test_part=test_size,\n",
    "                              seed=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  9017747\n",
      "Size of validation set:  3857476\n",
      "Size of test set:  1 (not converted)\n"
     ]
    }
   ],
   "source": [
    "kc_y_train = kc_data.loc[kc_train, [label_col]]\n",
    "kc_x_train = kc_data.loc[kc_train, :].drop(label_col, axis=1)\n",
    "kc_y_valid = kc_data.loc[kc_valid, [label_col]]\n",
    "kc_x_valid = kc_data.loc[kc_valid, :].drop(label_col, axis=1)\n",
    "\n",
    "print('Size of training set: ', len(kc_x_train))\n",
    "print('Size of validation set: ', len(kc_x_valid))\n",
    "print('Size of test set: ', len(kc_test), '(not converted)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_stats(df1, df2):\n",
    "    dfs = df1.append(df2)\n",
    "    minimum = np.min(dfs)\n",
    "    maximum = np.max(dfs)\n",
    "    mu = np.mean(dfs)\n",
    "    sigma = np.std(dfs)\n",
    "    return (minimum, maximum, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(col, stats):\n",
    "    m, M, mu, s = stats\n",
    "    df = pd.DataFrame()\n",
    "    for c in col.columns:\n",
    "        df[c] = (col[c]-mu[c])/s[c]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (9017747, 79)\n",
      "Training samples:  9017747\n",
      "Validation samples:  3857476\n"
     ]
    }
   ],
   "source": [
    "stats = norm_stats(kc_x_train, kc_x_valid)\n",
    "arr_x_train = np.array(z_score(kc_x_train, stats))\n",
    "arr_y_train = np.array(kc_y_train)\n",
    "arr_x_valid = np.array(z_score(kc_x_valid, stats))\n",
    "arr_y_valid = np.array(kc_y_valid)\n",
    "\n",
    "print('Training shape:', arr_x_train.shape)\n",
    "print('Training samples: ', arr_x_train.shape[0])\n",
    "print('Validation samples: ', arr_x_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "      <td>9.017747e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.659216e-04</td>\n",
       "      <td>-2.797203e-04</td>\n",
       "      <td>-4.130255e-05</td>\n",
       "      <td>6.094169e-04</td>\n",
       "      <td>-8.558740e-05</td>\n",
       "      <td>-9.801214e-04</td>\n",
       "      <td>-2.493424e-03</td>\n",
       "      <td>-8.677850e-03</td>\n",
       "      <td>3.432867e-03</td>\n",
       "      <td>4.820808e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>5.991297e-04</td>\n",
       "      <td>-2.791303e-04</td>\n",
       "      <td>-4.635210e-05</td>\n",
       "      <td>6.111175e-04</td>\n",
       "      <td>-7.922623e-05</td>\n",
       "      <td>1.020143e-03</td>\n",
       "      <td>-9.722583e-04</td>\n",
       "      <td>-2.498436e-03</td>\n",
       "      <td>-8.652522e-03</td>\n",
       "      <td>3.413229e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000045e+00</td>\n",
       "      <td>1.000036e+00</td>\n",
       "      <td>9.997072e-01</td>\n",
       "      <td>1.000901e+00</td>\n",
       "      <td>9.999152e-01</td>\n",
       "      <td>9.972574e-01</td>\n",
       "      <td>9.958553e-01</td>\n",
       "      <td>9.966305e-01</td>\n",
       "      <td>9.952755e-01</td>\n",
       "      <td>1.000154e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000860e+00</td>\n",
       "      <td>1.000037e+00</td>\n",
       "      <td>9.997098e-01</td>\n",
       "      <td>1.000898e+00</td>\n",
       "      <td>9.999189e-01</td>\n",
       "      <td>9.997258e-01</td>\n",
       "      <td>9.972605e-01</td>\n",
       "      <td>9.958691e-01</td>\n",
       "      <td>9.966412e-01</td>\n",
       "      <td>9.952903e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.436756e+00</td>\n",
       "      <td>-3.607960e+00</td>\n",
       "      <td>-1.249595e+00</td>\n",
       "      <td>-7.579588e+00</td>\n",
       "      <td>-3.233829e+00</td>\n",
       "      <td>-1.800122e+00</td>\n",
       "      <td>-1.448069e+00</td>\n",
       "      <td>-9.020263e-01</td>\n",
       "      <td>-1.907935e+00</td>\n",
       "      <td>-1.524504e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.994316e+00</td>\n",
       "      <td>-3.608432e+00</td>\n",
       "      <td>-1.249915e+00</td>\n",
       "      <td>-7.579834e+00</td>\n",
       "      <td>-3.234048e+00</td>\n",
       "      <td>-5.339462e+00</td>\n",
       "      <td>-1.799079e+00</td>\n",
       "      <td>-1.448310e+00</td>\n",
       "      <td>-9.020570e-01</td>\n",
       "      <td>-1.907941e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.554872e-01</td>\n",
       "      <td>-6.247605e-01</td>\n",
       "      <td>-5.883721e-01</td>\n",
       "      <td>-5.502265e-01</td>\n",
       "      <td>-6.295247e-01</td>\n",
       "      <td>-9.814363e-01</td>\n",
       "      <td>-6.019886e-01</td>\n",
       "      <td>-7.830451e-01</td>\n",
       "      <td>-3.341023e-02</td>\n",
       "      <td>-1.524504e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.623686e-01</td>\n",
       "      <td>-6.247533e-01</td>\n",
       "      <td>-5.884198e-01</td>\n",
       "      <td>-5.505319e-01</td>\n",
       "      <td>-6.294399e-01</td>\n",
       "      <td>-6.011989e-01</td>\n",
       "      <td>-9.806910e-01</td>\n",
       "      <td>-6.023415e-01</td>\n",
       "      <td>-7.830791e-01</td>\n",
       "      <td>-3.360180e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.390582e-02</td>\n",
       "      <td>-6.805552e-02</td>\n",
       "      <td>-3.351558e-01</td>\n",
       "      <td>-7.400682e-02</td>\n",
       "      <td>-5.440144e-02</td>\n",
       "      <td>2.662997e-01</td>\n",
       "      <td>6.019152e-02</td>\n",
       "      <td>-6.945529e-01</td>\n",
       "      <td>3.510465e-01</td>\n",
       "      <td>-1.524504e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.954433e-02</td>\n",
       "      <td>-6.770179e-02</td>\n",
       "      <td>-3.352111e-01</td>\n",
       "      <td>-7.440974e-02</td>\n",
       "      <td>-5.424955e-02</td>\n",
       "      <td>1.295969e-01</td>\n",
       "      <td>2.665911e-01</td>\n",
       "      <td>5.975077e-02</td>\n",
       "      <td>-6.945892e-01</td>\n",
       "      <td>3.508170e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.786480e-01</td>\n",
       "      <td>5.582606e-01</td>\n",
       "      <td>1.977197e-01</td>\n",
       "      <td>4.971000e-01</td>\n",
       "      <td>5.212230e-01</td>\n",
       "      <td>6.046289e-01</td>\n",
       "      <td>1.146673e+00</td>\n",
       "      <td>1.225233e+00</td>\n",
       "      <td>7.763853e-01</td>\n",
       "      <td>-1.524504e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.821305e-01</td>\n",
       "      <td>5.584578e-01</td>\n",
       "      <td>1.975679e-01</td>\n",
       "      <td>4.969797e-01</td>\n",
       "      <td>5.213969e-01</td>\n",
       "      <td>7.278314e-01</td>\n",
       "      <td>6.047973e-01</td>\n",
       "      <td>1.146088e+00</td>\n",
       "      <td>1.225145e+00</td>\n",
       "      <td>7.761139e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.836428e+00</td>\n",
       "      <td>6.540324e+00</td>\n",
       "      <td>1.000566e+01</td>\n",
       "      <td>8.076581e+00</td>\n",
       "      <td>7.926121e+00</td>\n",
       "      <td>1.460127e+00</td>\n",
       "      <td>1.529243e+00</td>\n",
       "      <td>1.530847e+00</td>\n",
       "      <td>8.834030e-01</td>\n",
       "      <td>6.559511e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.934375e+00</td>\n",
       "      <td>6.541482e+00</td>\n",
       "      <td>1.000664e+01</td>\n",
       "      <td>8.075723e+00</td>\n",
       "      <td>7.927203e+00</td>\n",
       "      <td>1.758029e+00</td>\n",
       "      <td>1.459984e+00</td>\n",
       "      <td>1.528607e+00</td>\n",
       "      <td>1.530750e+00</td>\n",
       "      <td>8.831210e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06   \n",
       "mean   3.659216e-04 -2.797203e-04 -4.130255e-05  6.094169e-04 -8.558740e-05   \n",
       "std    1.000045e+00  1.000036e+00  9.997072e-01  1.000901e+00  9.999152e-01   \n",
       "min   -3.436756e+00 -3.607960e+00 -1.249595e+00 -7.579588e+00 -3.233829e+00   \n",
       "25%   -6.554872e-01 -6.247605e-01 -5.883721e-01 -5.502265e-01 -6.295247e-01   \n",
       "50%    1.390582e-02 -6.805552e-02 -3.351558e-01 -7.400682e-02 -5.440144e-02   \n",
       "75%    6.786480e-01  5.582606e-01  1.977197e-01  4.971000e-01  5.212230e-01   \n",
       "max    4.836428e+00  6.540324e+00  1.000566e+01  8.076581e+00  7.926121e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06   \n",
       "mean  -9.801214e-04 -2.493424e-03 -8.677850e-03  3.432867e-03  4.820808e-05   \n",
       "std    9.972574e-01  9.958553e-01  9.966305e-01  9.952755e-01  1.000154e+00   \n",
       "min   -1.800122e+00 -1.448069e+00 -9.020263e-01 -1.907935e+00 -1.524504e-01   \n",
       "25%   -9.814363e-01 -6.019886e-01 -7.830451e-01 -3.341023e-02 -1.524504e-01   \n",
       "50%    2.662997e-01  6.019152e-02 -6.945529e-01  3.510465e-01 -1.524504e-01   \n",
       "75%    6.046289e-01  1.146673e+00  1.225233e+00  7.763853e-01 -1.524504e-01   \n",
       "max    1.460127e+00  1.529243e+00  1.530847e+00  8.834030e-01  6.559511e+00   \n",
       "\n",
       "           ...                 69            70            71            72  \\\n",
       "count      ...       9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06   \n",
       "mean       ...       5.991297e-04 -2.791303e-04 -4.635210e-05  6.111175e-04   \n",
       "std        ...       1.000860e+00  1.000037e+00  9.997098e-01  1.000898e+00   \n",
       "min        ...      -6.994316e+00 -3.608432e+00 -1.249915e+00 -7.579834e+00   \n",
       "25%        ...      -5.623686e-01 -6.247533e-01 -5.884198e-01 -5.505319e-01   \n",
       "50%        ...      -7.954433e-02 -6.770179e-02 -3.352111e-01 -7.440974e-02   \n",
       "75%        ...       4.821305e-01  5.584578e-01  1.975679e-01  4.969797e-01   \n",
       "max        ...       7.934375e+00  6.541482e+00  1.000664e+01  8.075723e+00   \n",
       "\n",
       "                 73            74            75            76            77  \\\n",
       "count  9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06  9.017747e+06   \n",
       "mean  -7.922623e-05  1.020143e-03 -9.722583e-04 -2.498436e-03 -8.652522e-03   \n",
       "std    9.999189e-01  9.997258e-01  9.972605e-01  9.958691e-01  9.966412e-01   \n",
       "min   -3.234048e+00 -5.339462e+00 -1.799079e+00 -1.448310e+00 -9.020570e-01   \n",
       "25%   -6.294399e-01 -6.011989e-01 -9.806910e-01 -6.023415e-01 -7.830791e-01   \n",
       "50%   -5.424955e-02  1.295969e-01  2.665911e-01  5.975077e-02 -6.945892e-01   \n",
       "75%    5.213969e-01  7.278314e-01  6.047973e-01  1.146088e+00  1.225145e+00   \n",
       "max    7.927203e+00  1.758029e+00  1.459984e+00  1.528607e+00  1.530750e+00   \n",
       "\n",
       "                 78  \n",
       "count  9.017747e+06  \n",
       "mean   3.413229e-03  \n",
       "std    9.952903e-01  \n",
       "min   -1.907941e+00  \n",
       "25%   -3.360180e-02  \n",
       "50%    3.508170e-01  \n",
       "75%    7.761139e-01  \n",
       "max    8.831210e-01  \n",
       "\n",
       "[8 rows x 79 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(arr_x_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model_1(x_size, y_size):\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
    "    t_model.add(Dense(50, activation=\"relu\"))\n",
    "    t_model.add(Dense(y_size))\n",
    "    print(t_model.summary())\n",
    "    t_model.compile(loss='mean_squared_error',\n",
    "        optimizer=Adam(),\n",
    "        metrics=[metrics.mae])\n",
    "    return(t_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model_2(x_size, y_size):\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
    "    t_model.add(Dropout(0.1))\n",
    "    t_model.add(Dense(50, activation=\"relu\"))\n",
    "    t_model.add(Dense(20, activation=\"relu\"))\n",
    "    t_model.add(Dense(y_size))\n",
    "    print(t_model.summary())\n",
    "    t_model.compile(loss='mean_squared_error',\n",
    "        optimizer=Adam(),\n",
    "        metrics=[metrics.mae])\n",
    "    return(t_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model_3(x_size, y_size):\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(80, activation=\"tanh\", kernel_initializer='normal', input_shape=(x_size,)))\n",
    "    t_model.add(Dropout(0.2))\n",
    "    t_model.add(Dense(120, activation=\"relu\", kernel_initializer='normal', \n",
    "        kernel_regularizer=regularizers.l1(0.01), bias_regularizer=regularizers.l1(0.01)))\n",
    "    t_model.add(Dropout(0.1))\n",
    "    t_model.add(Dense(20, activation=\"relu\", kernel_initializer='normal', \n",
    "        kernel_regularizer=regularizers.l1_l2(0.01), bias_regularizer=regularizers.l1_l2(0.01)))\n",
    "    t_model.add(Dropout(0.1))\n",
    "    t_model.add(Dense(10, activation=\"relu\", kernel_initializer='normal'))\n",
    "    t_model.add(Dropout(0.0))\n",
    "    t_model.add(Dense(y_size))\n",
    "    t_model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='nadam',\n",
    "        metrics=[metrics.mae])\n",
    "    return(t_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               8000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,101\n",
      "Trainable params: 13,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               8000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,101\n",
      "Trainable params: 13,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = basic_model_1(arr_x_train.shape[1], arr_y_train.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  5\n",
      "Batch size:  5\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 5\n",
    "\n",
    "print('Epochs: ', epochs)\n",
    "print('Batch size: ', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_callbacks = [\n",
    "    # ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True, verbose=2)\n",
    "    # ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "    # TensorBoard(log_dir='/tmp/keras_logs/model_3', histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None),\n",
    "    EarlyStopping(monitor='val_mean_absolute_error', patience=20, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-80db5b634f2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Change it to 2, if wished to observe execution\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_x_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_y_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     callbacks=keras_callbacks)\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1221\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m                             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m                         raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_slice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(arr_x_train, arr_y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=0, # Change it to 2, if wished to observe execution\n",
    "    validation_data=(arr_x_valid, arr_y_valid),\n",
    "    callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\n",
    "valid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n",
    "\n",
    "print('Train MAE: ', round(train_score[1], 4), ', Train Loss: ', round(train_score[0], 4)) \n",
    "print('Val MAE: ', round(valid_score[1], 4), ', Val Loss: ', round(valid_score[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(h, xsize=6, ysize=10):\n",
    "    # Prepare plotting\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=4, sharex=True)\n",
    "    \n",
    "    # summarize history for MAE\n",
    "    plt.subplot(211)\n",
    "    plt.plot(h['mean_absolute_error'])\n",
    "    plt.plot(h['val_mean_absolute_error'])\n",
    "    plt.title('Training vs Validation MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.subplot(212)\n",
    "    plt.plot(h['loss'])\n",
    "    plt.plot(h['val_loss'])\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot it all in IPython (non-interactive)\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(history.history, xsize=8, ysize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(kc_x_valid)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = arr_y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true, y_pred, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "explained_variance_score(y_true, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
